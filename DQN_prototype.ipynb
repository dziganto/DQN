{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from collections import deque\n",
    "import skimage.measure\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(rgb_array):\n",
    "    '''\n",
    "    Takes 3D RGB tensor and consolidates to 2D Gray-scale tensor.\n",
    "    '''\n",
    "    transform = rgb_array[30:194,:,:]  ## crop\n",
    "    gray = np.dot(transform[...,:3], [0.2989, 0.5870, 0.1140])  ## convert RGB to Gray Scale using Matlab's numbers\n",
    "    downsample = skimage.measure.block_reduce(gray, (2,2), np.max)  ## downsampling (aka max pooling)\n",
    "    return downsample\n",
    "\n",
    "def experience_replay(C, DQ, seq_init, action, reward, seq_update, gamestatus):\n",
    "    '''\n",
    "    Inputs:\n",
    "        C = capacity of experience replay (how big window should be)\n",
    "        DQ = deque object\n",
    "        seq_init = preprocessed frame before next action taken\n",
    "        action = action taken\n",
    "        reward = reward given action\n",
    "        seq_update = preprocessed frame after action taken (new observation)\n",
    "        gamestatus = whether end of game or not\n",
    "    Output:\n",
    "        updated DQ\n",
    "    '''\n",
    "    DQ.append((seq_init, action, reward, seq_update, gamestatus))\n",
    "    if len(DQ) > C:\n",
    "        DQ.popleft()\n",
    "    return DQ\n",
    "\n",
    "def epsilon_generator(start=1, stop=0.1, num=10):\n",
    "    '''\n",
    "    A generator that linearly anneals epsilon, which is the prob that a random action is taken\n",
    "    '''\n",
    "    test = 0\n",
    "    epsilon = start\n",
    "    step = (start - stop) / (num - 1)\n",
    "    while epsilon >= stop and test < num:    \n",
    "        yield epsilon\n",
    "        epsilon -= step\n",
    "        test += 1\n",
    "    else:\n",
    "        yield stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 8, 4)  ## Conv2d(nChannels, filters, kernel, stride)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, 4)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 32 * 4 * 4)  ## reshape \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(cnn.parameters(), \n",
    "                          lr=learning_rate, \n",
    "                          alpha=0.99, \n",
    "                          eps=1e-08, \n",
    "                          weight_decay=0, \n",
    "                          momentum=0, \n",
    "                          centered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "Conv2d(16, 32, kernel_size=(4, 4), stride=(4, 4))\n",
      "Linear (512 -> 256)\n",
      "Linear (256 -> 4)\n"
     ]
    }
   ],
   "source": [
    "print(cnn.conv1)\n",
    "print(cnn.conv2)\n",
    "print(cnn.fc1)\n",
    "print(cnn.fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ALGORITHM STEPS]\n",
    "\n",
    "Initialize replay memory D to capacity N  \n",
    "Initialize action-value function Q with random weights  \n",
    "for episode = 1, M do  \n",
    "    Initialise sequence s1 = {x1} and preprocessed sequenced φ1 = φ(s1)  \n",
    "    for t = 1, T do  \n",
    "        With probability \u000f select a random action at  \n",
    "        otherwise select a_t = maxa Q∗(φ(st), a; θ)  \n",
    "        Execute action a_t in emulator and observe reward rt and image xt+1  \n",
    "        Set st+1 = s_t, a_t, xt+1 and preprocess φt+1 = φ(s_t+1)  \n",
    "        Store transition (φt, a_t, r_t, φt+1) in D  \n",
    "        Sample random minibatch of transitions (φ_j , a_j , r_j , φj+1) from D  \n",
    "        Set y_j = r_j (for terminal φ_j+1)  \n",
    "                 r_j + γ maxa0 Q(φ_j+1, a0; θ) (for non-terminal φ_j+1)  \n",
    "        Perform a gradient descent step on (yj − Q(φj , aj ; θ))^2 according to equation 3  \n",
    "    end for  \n",
    "end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atari Emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-24 22:45:44,009] Making new env: Breakout-v0\n",
      "[2017-08-24 22:45:44,210] Clearing 9 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "env = wrappers.Monitor(env, \n",
    "                       directory='/Users/davidziganto/Data_Science/PyTorch/OpenAI_vids/breakout-experiment-1', \n",
    "                       video_callable=None, ## takes video when episode number is perfect cube\n",
    "                       force=True, \n",
    "                       resume=False, \n",
    "                       write_upon_reset=False, \n",
    "                       uid=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_games = 2  ## number of games to play\n",
    "time_steps = 1000  ## max number of time steps per game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'Experience Replay' Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = int(1e6)  ## capacity\n",
    "D = deque()  ## deque object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning Variables & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anneal_tracker = 0  ## tally of how many total iterations have passed\n",
    "anneal_stop = int(1e6)  ## nb of steps until annealing stops\n",
    "gen_epsilon = epsilon_generator(start=1, stop=0.1, num=anneal_stop)  ## Prob(choosing random action) w/linear annealing\n",
    "discount = 0.9  ## on future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-24 22:45:44,797] Starting new video recorder writing to /Users/davidziganto/Data_Science/PyTorch/OpenAI_vids/breakout-experiment-1/openaigym.video.0.1225.video000000.mp4\n",
      "[2017-08-24 22:45:44,846] Starting new video recorder writing to /Users/davidziganto/Data_Science/PyTorch/OpenAI_vids/breakout-experiment-1/openaigym.video.0.1225.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 63615.008\n",
      "[2,   100] loss: 0.005\n",
      "[2,   200] loss: 0.008\n",
      "[2,   300] loss: 0.016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-24 22:48:58,065] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/davidziganto/Data_Science/PyTorch/OpenAI_vids/breakout-experiment-1')\n"
     ]
    }
   ],
   "source": [
    "## check nb times NN chooses action\n",
    "checker = 0\n",
    "\n",
    "for episode in range(nb_games):\n",
    "    ## reset environment\n",
    "    env.reset()\n",
    "    ## setup to check loss per episode\n",
    "    running_loss = 0.0\n",
    "    ## empty list to capture mini-batch of frames\n",
    "    frames = []\n",
    "    ## default status to differentiate rewards (aka targets)\n",
    "    gamestatus = 'nonterminal'  \n",
    "    ## raw frame of game start\n",
    "    raw_frame = env.reset()  \n",
    "    ## preprocessed initial frame \n",
    "    seq_init = preprocess(raw_frame)  \n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        ##print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            # feedforward for agent-based action\n",
    "            sample_frame = Variable(torch.Tensor(seq_init).unsqueeze(0).unsqueeze(0))  ## setup for CNN (unsqueeze to fake 4D tensor since single observation)\n",
    "            action_decision = cnn(sample_frame)  ## return optimal action \n",
    "            ##print(action_decision)\n",
    "            # take epsilon-greedy action (prob(epsilon) = random; else argmax(action))\n",
    "            #action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action_decision.data.max(1)[1][0]\n",
    "            if np.random.binomial(n=1, p=epsilon, size=1):\n",
    "                action = env.action_space.sample()\n",
    "            else: \n",
    "                checker += 1\n",
    "                action = action_decision.data.max(1)[1][0]\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # gather feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # preprocess new observation post action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # mini-batch setup\n",
    "        if t%4 == 3 or done:\n",
    "            ##print(t)\n",
    "            frames.append(seq_update)\n",
    "            ## makes arrays callable to feed into CNN\n",
    "            frameTensor = np.stack(frames)  \n",
    "            ## convert Numpy Array --> PyTorch Tensor --> PyTorch Variable\n",
    "            frameTensor = Variable(torch.Tensor(frameTensor))  \n",
    "            ##print('t:', t, '\\n', frameTensor)  ## should be 4x82x80 unless 'done'\n",
    "            ## clear mini-batch\n",
    "            frames = []  \n",
    "        else:\n",
    "            frames.append(seq_update)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            ##print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "            ##      '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            ##print('step:', t, '| gamestatus:', gamestatus, '| action:', action, '| len(D):', len(D), \n",
    "            ##      '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        ## empty list to capture mini-batch of D\n",
    "        minibatch_D = []\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            minibatch_D.append(D[i])\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2]) \n",
    "            \n",
    "        # create dataset\n",
    "        data_list = [D[i][0] for i in range(D_size)]\n",
    "        data = Variable(torch.Tensor(data_list).unsqueeze(1))\n",
    "        ##print(data)\n",
    "        \n",
    "        # create target variable\n",
    "        target_list = []\n",
    "        for i in range(D_size):\n",
    "            if D[i][4] == 'terminal':\n",
    "                target_list.append(D[i][2])\n",
    "            else:\n",
    "                target_list.append(D[i][2] + discount * \n",
    "                                   cnn(Variable(torch.Tensor(D[i][3]).unsqueeze(0).unsqueeze(0))).data.max(1)[1][0])\n",
    "        targets = Variable(torch.Tensor(target_list))\n",
    "        ##print(targets)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feedforward pass\n",
    "        outputs = cnn(data).max(1)[0]\n",
    "        ##print(outputs)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        #print('loss:', loss)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if t % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (episode + 1, t + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(minibatch_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(cnn.state_dict(), '/Users/davidziganto/Data_Science/PyTorch/DL_models/DL_RL_Atari_breakout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn = CNN()\n",
    "#cnn.load_state_dict(torch.load('/Users/davidziganto/Data_Science/PyTorch/DL_models/DL_RL_Atari_breakout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE\n",
    "\n",
    "### Get Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "rewards = []\n",
    "nb_frames = 500\n",
    "env = gym.make('Breakout-v0')\n",
    "env.reset()\n",
    "for t in range(nb_frames):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # take a random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frames.append(preprocess(observation))\n",
    "    if t%4 == 3 or done:\n",
    "        frameTensor = np.stack(frames)\n",
    "        minibatch = Variable(torch.Tensor(frameTensor))  ## convert to torch Variable data type\n",
    "        print('t:', t, '\\n', minibatch)\n",
    "        frames = []\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Preprocessed Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for frame in frames:\n",
    "    plt.imshow(frame, cmap = plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# EXPERIMENTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "# Atari emulator\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "# game variables\n",
    "nb_games = 5  ## number of games to play\n",
    "time_steps = 500  ## max number of time steps per game\n",
    "\n",
    "# experience replay variables\n",
    "N = int(1e6)  ## capacity\n",
    "D = deque()  ## deque object\n",
    "\n",
    "# RL vars\n",
    "anneal_tracker = 0  ## tally of how many total iterations have passed\n",
    "anneal_stop = 1000  ## nb of steps until annealing stops\n",
    "gen_epsilon = epsilon_generator(start=1, stop=0.1, num=anneal_stop)  ## Prob(choosing random action) w/linear annealing\n",
    "discount = 0.9  ## on future rewards\n",
    "\n",
    "# CNN setup\n",
    "cnn = CNN()\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(cnn.params, \n",
    "                          lr=learning_rate, \n",
    "                          alpha=0.99, \n",
    "                          eps=1e-08, \n",
    "                          weight_decay=0, \n",
    "                          momentum=0, \n",
    "                          centered=False)\n",
    "\n",
    "# algorithm\n",
    "for episode in range(nb_games):\n",
    "    gamestatus = 'nonterminal'\n",
    "    raw_frame = env.reset()  ## raw initial frame\n",
    "    seq_init = preprocess(raw_frame)  ## preprocessed initial sequence \n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            action = env.action_space.sample() # take a random action\n",
    "            #action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action w/max Q-value\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # preprocess new observation after action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('step:', t, '| gamestatus:', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            if D[i][4] == 'terminal':\n",
    "                target = D[i][2] + 100\n",
    "            else:\n",
    "                #target = sample[i][2] + discount*(to be completed)\n",
    "                target = D[i][2]\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2])\n",
    "        # SGD update\n",
    "        #update weights\n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        #print('final target =', target)\n",
    "    #print( (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    #print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "for episode in range(nb_games):\n",
    "    ## setup to check loss per episode\n",
    "    running_loss = 0.0\n",
    "    ## empty list to capture mini-batch of frames\n",
    "    frames = []  \n",
    "    ## default status to differentiate rewards (aka targets)\n",
    "    gamestatus = 'nonterminal'  \n",
    "    ## raw frame of game start\n",
    "    raw_frame = env.reset()  \n",
    "    ## preprocessed initial frame \n",
    "    seq_init = preprocess(raw_frame)  \n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            # feedforward for agent-based action\n",
    "            action_decision = Variable(torch.Tensor(seq_init))  ## setup for CNN\n",
    "            action_decision = cnn(action_decision.unsqueeze(0))  ## return optimal action\n",
    "            # take epsilon-greedy action (prob(epsilon) = random; else argmax(action))\n",
    "            action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action_decision.data.max()\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # gather feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # preprocess new observation post action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # mini-batch setup\n",
    "        if t%4 == 3  or done:\n",
    "            ## makes arrays callable to feed into CNN\n",
    "            frameTensor = np.stack(frames)  \n",
    "            ## convert Numpy Array --> PyTorch Tensor --> PyTorch Variable\n",
    "            frameTensor = Variable(torch.Tensor(frameTensor))  \n",
    "            print('t:', t, '\\n', frameTensor.shape)  ## should be 4x82x80 unless 'done'\n",
    "            ## clear mini-batch\n",
    "            frames = []  \n",
    "        else:\n",
    "            frames.append(seq_update)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('step:', t, '| gamestatus:', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            if D[i][4] == 'terminal':\n",
    "                #target = D[i][2] + (discount * )\n",
    "                target = D[i][2]\n",
    "            else:\n",
    "                target = D[i][2]\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2])\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feedforward\n",
    "        outputs = cnn(frameTensor)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        print('loss:', loss)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if t % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (episode + 1, t + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        \n",
    "        #print('final target =', target)\n",
    "    #print( (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    #print(D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
