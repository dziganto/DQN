{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from collections import deque\n",
    "import skimage.measure\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data_ = data\n",
    "        \n",
    "    def get_data(self, data):\n",
    "        self.data_ = data\n",
    "        \n",
    "    def crop(self, h_start=30, h_end=194):\n",
    "        self.data_ = self.data_[h_start:h_end, ::]\n",
    "        \n",
    "    def rgb2gray(self):\n",
    "        self.data_ = np.dot(self.data_, [0.2989, 0.5870, 0.1140])\n",
    "        \n",
    "    def downsample(self, kernel=2):\n",
    "        self.data_ = skimage.measure.block_reduce(self.data_, (kernel, kernel), np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    \n",
    "    dq_ = deque(maxlen=32)\n",
    "\n",
    "    def __init__(self, C, experience_tuple):\n",
    "        self.capacity_ = C\n",
    "        self.exp_tuple_ = experience_tuple\n",
    "        self.dq_.append(experience_tuple)\n",
    "        \n",
    "    def add_experience(self, experience_tuple):\n",
    "        '''add new experience'''\n",
    "        self.dq_.append(experience_tuple)\n",
    "    \n",
    "    def sample(self, capacity):\n",
    "        '''sample from experience'''\n",
    "        nb_items = len(self.dq_)\n",
    "        if nb_items > capacity:\n",
    "            idx = np.random.choice( nb_items, size=capacity, replace=False)\n",
    "        else:\n",
    "            idx = np.random.choice( nb_items, size=nb_items, replace=False)\n",
    "        return [self.dq_[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpsilonGenerator():\n",
    "    \n",
    "    def __init__(self, start, stop, steps):\n",
    "        self.epsilon_ = start\n",
    "        self.stop_ = stop\n",
    "        self.steps_ = steps\n",
    "        self.step_size_ = (self.epsilon_ - stop) / (self.steps_)\n",
    "        self.count_ = 0\n",
    "        \n",
    "    def epsilon_update(self):\n",
    "        if self.count_ == 0:\n",
    "            self.count_ += 1\n",
    "            return self.epsilon_\n",
    "        elif (self.epsilon_ >= self.stop_ and self.count_ < self.steps_):\n",
    "            self.count_ += 1\n",
    "            self.epsilon_ -= self.step_size_\n",
    "        else:\n",
    "            self.epsilon_ = self.stop_\n",
    "            self.count_ += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 8, 4)  ## Conv2d(nChannels, filters, kernel, stride)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, 4)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 32 * 4 * 4)  ## reshape \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_games = 1  ## number of games to play\n",
    "time_steps = 100  ## max number of time steps per game\n",
    "record = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-28 16:29:46,398] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "if record:\n",
    "    env = wrappers.Monitor(env, \n",
    "                           directory='/Users/davidziganto/Data_Science/PyTorch/OpenAI_vids/breakout-experiment-1', \n",
    "                           video_callable=None, ## takes video when episode number is perfect cube\n",
    "                           force=True)\n",
    "\n",
    "# setup for CNN\n",
    "cnn = CNN()\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(cnn.parameters(), \n",
    "                          lr=learning_rate, \n",
    "                          alpha=0.99, \n",
    "                          eps=1e-08, \n",
    "                          weight_decay=0, \n",
    "                          momentum=0, \n",
    "                          centered=False)\n",
    "\n",
    "# variables\n",
    "discount = 0.9  ## on future rewards\n",
    "\n",
    "# play game\n",
    "for episode in range(num_games):\n",
    "    \n",
    "    ## reset environment\n",
    "    initial_seq = Preprocess(env.reset())\n",
    "\n",
    "    # preprocess initial sequence\n",
    "    initial_seq.crop()\n",
    "    initial_seq.rgb2gray()\n",
    "    initial_seq.downsample()\n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        ## show game in real-time\n",
    "        env.render()\n",
    "        \n",
    "        # take action (0=do nothing; 1=fire ball; 2=move right; 3=move left)\n",
    "        if t == 0:\n",
    "            action = 1\n",
    "        else:\n",
    "            if t % 4 == 0:\n",
    "                action = env.action_space.sample()\n",
    "        \n",
    "        # get feedback\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats for 1000 games taking random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-28 16:47:04,100] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "rewards = []\n",
    "steps = []\n",
    "\n",
    "for game in range(1000):\n",
    "    \n",
    "    myreward = 0\n",
    "    \n",
    "    # reset game\n",
    "    env.reset()\n",
    "    \n",
    "    for t in range(1000):\n",
    "        \n",
    "        # show in real-time\n",
    "        #env.render()\n",
    "        \n",
    "        # take a random action\n",
    "        if t == 0:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = env.action_space.sample() \n",
    "        \n",
    "        # get feedback\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        myreward += reward\n",
    "        \n",
    "        # end game when out of balls\n",
    "        if done:\n",
    "            rewards.append(myreward)\n",
    "            steps.append(t)\n",
    "            stats = zip(steps, rewards)\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nu4iIByncRUQ8SOEuIuJB/x8Y/6gQ3q3hnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121d9a550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(rewards);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        #env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        ##print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            # feedforward for agent-based action\n",
    "            sample_frame = Variable(torch.Tensor(seq_init).unsqueeze(0).unsqueeze(0))  ## setup for CNN (unsqueeze to fake 4D tensor since single observation)\n",
    "            action_decision = cnn(sample_frame)  ## return optimal action \n",
    "            ##print(action_decision)\n",
    "            # take epsilon-greedy action (prob(epsilon) = random; else argmax(action))\n",
    "            #action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action_decision.data.max(1)[1][0]\n",
    "            if np.random.binomial(n=1, p=epsilon, size=1):\n",
    "                action = env.action_space.sample()\n",
    "            else: \n",
    "                checker += 1\n",
    "                action = action_decision.data.max(1)[1][0]\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # gather feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        # preprocess new observation post action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # mini-batch setup\n",
    "        if t%4 == 3 or done:\n",
    "            ##print(t)\n",
    "            frames.append(seq_update)\n",
    "            ## makes arrays callable to feed into CNN\n",
    "            frameTensor = np.stack(frames)  \n",
    "            ## convert Numpy Array --> PyTorch Tensor --> PyTorch Variable\n",
    "            frameTensor = Variable(torch.Tensor(frameTensor))  \n",
    "            ##print('t:', t, '\\n', frameTensor)  ## should be 4x82x80 unless 'done'\n",
    "            ## clear mini-batch\n",
    "            frames = []  \n",
    "        else:\n",
    "            frames.append(seq_update)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            ##print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "            ##      '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            print('steps:', t, '| episode:', episode, '| score:', score, '| checker:', checker)\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            ##print('step:', t, '| gamestatus:', gamestatus, '| action:', action, '| len(D):', len(D), \n",
    "            ##      '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        ## empty list to capture mini-batch of D\n",
    "        minibatch_D = []\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            minibatch_D.append(D[i])\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2]) \n",
    "            \n",
    "        # create dataset\n",
    "        data_list = [D[i][0] for i in range(D_size)]\n",
    "        data = Variable(torch.Tensor(data_list).unsqueeze(1))\n",
    "        ##print(data)\n",
    "        \n",
    "        # create target variable\n",
    "        target_list = []\n",
    "        for i in range(D_size):\n",
    "            if D[i][4] == 'terminal':\n",
    "                target_list.append(D[i][2])\n",
    "            else:\n",
    "                target_list.append(D[i][2] + discount * \n",
    "                                   cnn(Variable(torch.Tensor(D[i][3]).unsqueeze(0).unsqueeze(0))).data.max(1)[1][0])\n",
    "        targets = Variable(torch.Tensor(target_list))\n",
    "        ##print(targets)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feedforward pass\n",
    "        outputs = cnn(data).max(1)[0]\n",
    "        ##print(outputs)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        #print('loss:', loss)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        \n",
    "        # print statistics\n",
    "        #running_loss += loss.data[0]\n",
    "        #if t % 200 == 199:    # print every 200 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (episode + 1, t + 1, running_loss / 200))\n",
    "        #    running_loss = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check how many times DQN chose action as opposed to random action\n",
    "checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensure minibatch of experience replay doesn't exceed 32\n",
    "len(minibatch_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(cnn.state_dict(), '/Users/davidziganto/Data_Science/PyTorch/DL_models/DL_RL_Atari_breakout_500e_10000t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn = CNN()\n",
    "#cnn.load_state_dict(torch.load('/Users/davidziganto/Data_Science/PyTorch/DL_models/DL_RL_Atari_breakout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE\n",
    "\n",
    "### Get Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "rewards = []\n",
    "nb_frames = 500\n",
    "env = gym.make('Breakout-v0')\n",
    "env.reset()\n",
    "for t in range(nb_frames):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # take a random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frames.append(preprocess(observation))\n",
    "    if t%4 == 3 or done:\n",
    "        frameTensor = np.stack(frames)\n",
    "        minibatch = Variable(torch.Tensor(frameTensor))  ## convert to torch Variable data type\n",
    "        print('t:', t, '\\n', minibatch)\n",
    "        frames = []\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Preprocessed Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for frame in frames:\n",
    "    plt.imshow(frame, cmap = plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# EXPERIMENTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "# Atari emulator\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "# game variables\n",
    "nb_games = 5  ## number of games to play\n",
    "time_steps = 500  ## max number of time steps per game\n",
    "\n",
    "# experience replay variables\n",
    "N = int(1e6)  ## capacity\n",
    "D = deque()  ## deque object\n",
    "\n",
    "# RL vars\n",
    "anneal_tracker = 0  ## tally of how many total iterations have passed\n",
    "anneal_stop = 1000  ## nb of steps until annealing stops\n",
    "gen_epsilon = epsilon_generator(start=1, stop=0.1, num=anneal_stop)  ## Prob(choosing random action) w/linear annealing\n",
    "discount = 0.9  ## on future rewards\n",
    "\n",
    "# CNN setup\n",
    "cnn = CNN()\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(cnn.params, \n",
    "                          lr=learning_rate, \n",
    "                          alpha=0.99, \n",
    "                          eps=1e-08, \n",
    "                          weight_decay=0, \n",
    "                          momentum=0, \n",
    "                          centered=False)\n",
    "\n",
    "# algorithm\n",
    "for episode in range(nb_games):\n",
    "    gamestatus = 'nonterminal'\n",
    "    raw_frame = env.reset()  ## raw initial frame\n",
    "    seq_init = preprocess(raw_frame)  ## preprocessed initial sequence \n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            action = env.action_space.sample() # take a random action\n",
    "            #action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action w/max Q-value\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # preprocess new observation after action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('step:', t, '| gamestatus:', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            if D[i][4] == 'terminal':\n",
    "                target = D[i][2] + 100\n",
    "            else:\n",
    "                #target = sample[i][2] + discount*(to be completed)\n",
    "                target = D[i][2]\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2])\n",
    "        # SGD update\n",
    "        #update weights\n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        #print('final target =', target)\n",
    "    #print( (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    #print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "for episode in range(nb_games):\n",
    "    ## setup to check loss per episode\n",
    "    running_loss = 0.0\n",
    "    ## empty list to capture mini-batch of frames\n",
    "    frames = []  \n",
    "    ## default status to differentiate rewards (aka targets)\n",
    "    gamestatus = 'nonterminal'  \n",
    "    ## raw frame of game start\n",
    "    raw_frame = env.reset()  \n",
    "    ## preprocessed initial frame \n",
    "    seq_init = preprocess(raw_frame)  \n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            # feedforward for agent-based action\n",
    "            action_decision = Variable(torch.Tensor(seq_init))  ## setup for CNN\n",
    "            action_decision = cnn(action_decision.unsqueeze(0))  ## return optimal action\n",
    "            # take epsilon-greedy action (prob(epsilon) = random; else argmax(action))\n",
    "            action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action_decision.data.max()\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # gather feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # preprocess new observation post action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # mini-batch setup\n",
    "        if t%4 == 3  or done:\n",
    "            ## makes arrays callable to feed into CNN\n",
    "            frameTensor = np.stack(frames)  \n",
    "            ## convert Numpy Array --> PyTorch Tensor --> PyTorch Variable\n",
    "            frameTensor = Variable(torch.Tensor(frameTensor))  \n",
    "            print('t:', t, '\\n', frameTensor.shape)  ## should be 4x82x80 unless 'done'\n",
    "            ## clear mini-batch\n",
    "            frames = []  \n",
    "        else:\n",
    "            frames.append(seq_update)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('step:', t, '| gamestatus:', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            if D[i][4] == 'terminal':\n",
    "                #target = D[i][2] + (discount * )\n",
    "                target = D[i][2]\n",
    "            else:\n",
    "                target = D[i][2]\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2])\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feedforward\n",
    "        outputs = cnn(frameTensor)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        print('loss:', loss)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if t % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (episode + 1, t + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        \n",
    "        #print('final target =', target)\n",
    "    #print( (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    #print(D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
