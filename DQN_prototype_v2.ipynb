{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from collections import deque\n",
    "import skimage.measure\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-28 14:54:42,354] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "nb_frames = 1\n",
    "env = gym.make('Breakout-v0')\n",
    "env.reset()\n",
    "for t in range(nb_frames):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # take a random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frame = observation\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data_ = data\n",
    "        \n",
    "    def crop(self, h_start=30, h_end=194):\n",
    "        self.data_ = self.data_[h_start:h_end, ::]\n",
    "        \n",
    "    def rgb2gray(self):\n",
    "        self.data_ = np.dot(self.data_, [0.2989, 0.5870, 0.1140])\n",
    "        \n",
    "    def downsample(self, kernel=2):\n",
    "        self.data_ = skimage.measure.block_reduce(self.data_, (kernel, kernel), np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n20zMMtS4TwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10306a860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show raw data \n",
    "plt.imshow(frame, cmap = plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Preprocess(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop top and bottomw\n",
    "x.crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAD8CAYAAACYVXqwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114706080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.data_, cmap = plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 160, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to grayscale\n",
    "x.rgb2gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 160)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAD8CAYAAACYVXqwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGmYASA37P7QSTOeAAnluAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111a8f7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.data_, cmap = plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample w/max pooling\n",
    "x.downsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAAD8CAYAAAC1ggIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4EcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11477db70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x.data_, cmap = plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 80)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    \n",
    "    dq_ = deque(maxlen=32)\n",
    "\n",
    "    def __init__(self, C, experience_tuple):\n",
    "        self.capacity_ = C\n",
    "        self.exp_tuple_ = experience_tuple\n",
    "        self.dq_.append(experience_tuple)\n",
    "        \n",
    "        #self.seq_init = self.exp_tuple[0]\n",
    "        #self.action = self.exp_tuple[1]\n",
    "        #self.reward = self.exp_tuple[2]\n",
    "        #self.seq_update = self.exp_tuple[3]\n",
    "        #self.gamestatus = self.exp_tuple[4]\n",
    "        \n",
    "    def add_exp(self, experience_tuple):\n",
    "        '''add new experience'''\n",
    "        self.dq_.append(experience_tuple)\n",
    "    \n",
    "    def sample(self, capacity):\n",
    "        '''sample from experience'''\n",
    "        nb_items = len(self.dq_)\n",
    "        if nb_items > capacity:\n",
    "            idx = np.random.choice( nb_items, size=capacity, replace=False)\n",
    "        else:\n",
    "            idx = np.random.choice( nb_items, size=nb_items, replace=False)\n",
    "        return [self.dq_[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = (('init', 'action', 'reward', 'out', 'status'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('init', 'action', 'reward', 'out', 'status')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = ExperienceReplay(10, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([('init', 'action', 'reward', 'out', 'status')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er.dq_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "er.add_exp( ('init2', 'action2', 'reward2', 'out2', 'status2') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "er.add_exp( ('init4', 'action2', 'reward2', 'init3', 'status4') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([('init', 'action', 'reward', 'out', 'status'),\n",
       "       ('init2', 'action2', 'reward2', 'out2', 'status2'),\n",
       "       ('init4', 'action2', 'reward2', 'init3', 'status4')])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er.dq_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpsilonGenerator():\n",
    "    \n",
    "    def __init__(self, start, stop, steps):\n",
    "        self.epsilon_ = start\n",
    "        self.stop_ = stop\n",
    "        self.steps_ = steps\n",
    "        self.step_size_ = (self.epsilon_ - stop) / (self.steps_)\n",
    "        self.count_ = 0\n",
    "        \n",
    "    def epsilon_update(self):\n",
    "        if self.count_ == 0:\n",
    "            self.count_ += 1\n",
    "            return self.epsilon_\n",
    "        elif (self.epsilon_ >= self.stop_ and self.count_ < self.steps_):\n",
    "            self.count_ += 1\n",
    "            self.epsilon_ -= self.step_size_\n",
    "        else:\n",
    "            self.epsilon_ = self.stop_\n",
    "            self.count_ += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EpsilonGenerator Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = EpsilonGenerator(1, 0.1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg.epsilon_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg.steps_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 0.91\n",
      "3 0.8200000000000001\n",
      "4 0.7300000000000001\n",
      "5 0.6400000000000001\n",
      "6 0.5500000000000002\n",
      "7 0.4600000000000002\n",
      "8 0.3700000000000002\n",
      "9 0.28000000000000025\n",
      "10 0.19000000000000025\n",
      "11 0.1\n",
      "12 0.1\n",
      "13 0.1\n",
      "14 0.1\n",
      "15 0.1\n",
      "16 0.1\n",
      "17 0.1\n",
      "18 0.1\n",
      "19 0.1\n",
      "20 0.1\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    eg.epsilon_update()\n",
    "    print(eg.count_, eg.epsilon_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 8, 4)  ## Conv2d(nChannels, filters, kernel, stride)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, 4)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 32 * 4 * 4)  ## reshape \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(cnn.parameters(), \n",
    "                          lr=learning_rate, \n",
    "                          alpha=0.99, \n",
    "                          eps=1e-08, \n",
    "                          weight_decay=0, \n",
    "                          momentum=0, \n",
    "                          centered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "Conv2d(16, 32, kernel_size=(4, 4), stride=(4, 4))\n",
      "Linear (512 -> 256)\n",
      "Linear (256 -> 4)\n"
     ]
    }
   ],
   "source": [
    "print(cnn.conv1)\n",
    "print(cnn.conv2)\n",
    "print(cnn.fc1)\n",
    "print(cnn.fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRL(Preprocess, ExperienceReplay):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data_ = data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drl = DRL(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([('init', 'action', 'reward', 'out', 'status'),\n",
       "       ('init2', 'action2', 'reward2', 'out2', 'status2'),\n",
       "       ('init4', 'action2', 'reward2', 'init3', 'status4')])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drl.dq_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "drl.dq_.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drl.data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "drl.crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 160, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drl.data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "drl.rgb2gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 160)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drl.data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drl.downsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 80)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drl.data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drl.dq_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "drl.add_exp(('init2', 'action2', 'reward2', 'out2', 'status2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([('init2', 'action2', 'reward2', 'out2', 'status2')])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drl.dq_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('init2', 'action2', 'reward2', 'out2', 'status2')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drl.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "drl.add_exp((x.data_, 0, 1, x.data_, 'nonterminal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ALGORITHM STEPS]\n",
    "\n",
    "Initialize replay memory D to capacity N  \n",
    "Initialize action-value function Q with random weights  \n",
    "for episode = 1, M do  \n",
    "    Initialise sequence s1 = {x1} and preprocessed sequenced φ1 = φ(s1)  \n",
    "    for t = 1, T do  \n",
    "        With probability \u000f select a random action at  \n",
    "        otherwise select a_t = maxa Q∗(φ(st), a; θ)  \n",
    "        Execute action a_t in emulator and observe reward rt and image xt+1  \n",
    "        Set st+1 = s_t, a_t, xt+1 and preprocess φt+1 = φ(s_t+1)  \n",
    "        Store transition (φt, a_t, r_t, φt+1) in D  \n",
    "        Sample random minibatch of transitions (φ_j , a_j , r_j , φj+1) from D  \n",
    "        Set y_j = r_j (for terminal φ_j+1)  \n",
    "                 r_j + γ maxa0 Q(φ_j+1, a0; θ) (for non-terminal φ_j+1)  \n",
    "        Perform a gradient descent step on (yj − Q(φj , aj ; θ))^2 according to equation 3  \n",
    "    end for  \n",
    "end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atari Emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "env = wrappers.Monitor(env, \n",
    "                       directory='/Users/davidziganto/Data_Science/PyTorch/OpenAI_vids/breakout-experiment-1', \n",
    "                       video_callable=None, ## takes video when episode number is perfect cube\n",
    "                       force=True, \n",
    "                       resume=False, \n",
    "                       write_upon_reset=False, \n",
    "                       uid=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_games = 34  ## number of games to play\n",
    "time_steps = 2000  ## max number of time steps per game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning Variables & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#anneal_tracker = 0  ## tally of how many total iterations have passed\n",
    "#anneal_stop = int(1e4)  ## nb of steps until annealing stops\n",
    "#gen_epsilon = epsilon_generator(start=1, stop=0.1, num=anneal_stop)  ## Prob(choosing random action) w/linear annealing\n",
    "discount = 0.9  ## on future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(nb_games):\n",
    "    ## reset environment\n",
    "    env.reset()\n",
    "    ## check nb times NN chooses action\n",
    "    checker = 0\n",
    "    ## setup to check loss & score per episode\n",
    "    running_loss = 0.0\n",
    "    score = 0\n",
    "    ## empty list to capture mini-batch of frames\n",
    "    frames = []\n",
    "    ## default status to differentiate rewards (aka targets)\n",
    "    gamestatus = 'nonterminal'  \n",
    "    ## raw frame of game start\n",
    "    raw_frame = env.reset()  \n",
    "    ## preprocessed initial frame \n",
    "    seq_init = preprocess(raw_frame)  \n",
    "    \n",
    "    #print epsisode\n",
    "    #print('episode:', episode, 'checker:', checker)\n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        #env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        ##print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            # feedforward for agent-based action\n",
    "            sample_frame = Variable(torch.Tensor(seq_init).unsqueeze(0).unsqueeze(0))  ## setup for CNN (unsqueeze to fake 4D tensor since single observation)\n",
    "            action_decision = cnn(sample_frame)  ## return optimal action \n",
    "            ##print(action_decision)\n",
    "            # take epsilon-greedy action (prob(epsilon) = random; else argmax(action))\n",
    "            #action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action_decision.data.max(1)[1][0]\n",
    "            if np.random.binomial(n=1, p=epsilon, size=1):\n",
    "                action = env.action_space.sample()\n",
    "            else: \n",
    "                checker += 1\n",
    "                action = action_decision.data.max(1)[1][0]\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # gather feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        # preprocess new observation post action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # mini-batch setup\n",
    "        if t%4 == 3 or done:\n",
    "            ##print(t)\n",
    "            frames.append(seq_update)\n",
    "            ## makes arrays callable to feed into CNN\n",
    "            frameTensor = np.stack(frames)  \n",
    "            ## convert Numpy Array --> PyTorch Tensor --> PyTorch Variable\n",
    "            frameTensor = Variable(torch.Tensor(frameTensor))  \n",
    "            ##print('t:', t, '\\n', frameTensor)  ## should be 4x82x80 unless 'done'\n",
    "            ## clear mini-batch\n",
    "            frames = []  \n",
    "        else:\n",
    "            frames.append(seq_update)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            ##print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "            ##      '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            print('steps:', t, '| episode:', episode, '| score:', score, '| checker:', checker)\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            ##print('step:', t, '| gamestatus:', gamestatus, '| action:', action, '| len(D):', len(D), \n",
    "            ##      '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        ## empty list to capture mini-batch of D\n",
    "        minibatch_D = []\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            minibatch_D.append(D[i])\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2]) \n",
    "            \n",
    "        # create dataset\n",
    "        data_list = [D[i][0] for i in range(D_size)]\n",
    "        data = Variable(torch.Tensor(data_list).unsqueeze(1))\n",
    "        ##print(data)\n",
    "        \n",
    "        # create target variable\n",
    "        target_list = []\n",
    "        for i in range(D_size):\n",
    "            if D[i][4] == 'terminal':\n",
    "                target_list.append(D[i][2])\n",
    "            else:\n",
    "                target_list.append(D[i][2] + discount * \n",
    "                                   cnn(Variable(torch.Tensor(D[i][3]).unsqueeze(0).unsqueeze(0))).data.max(1)[1][0])\n",
    "        targets = Variable(torch.Tensor(target_list))\n",
    "        ##print(targets)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feedforward pass\n",
    "        outputs = cnn(data).max(1)[0]\n",
    "        ##print(outputs)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        #print('loss:', loss)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        \n",
    "        # print statistics\n",
    "        #running_loss += loss.data[0]\n",
    "        #if t % 200 == 199:    # print every 200 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (episode + 1, t + 1, running_loss / 200))\n",
    "        #    running_loss = 0.0\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check how many times DQN chose action as opposed to random action\n",
    "checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensure minibatch of experience replay doesn't exceed 32\n",
    "len(minibatch_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(cnn.state_dict(), '/Users/davidziganto/Data_Science/PyTorch/DL_models/DL_RL_Atari_breakout_500e_10000t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn = CNN()\n",
    "#cnn.load_state_dict(torch.load('/Users/davidziganto/Data_Science/PyTorch/DL_models/DL_RL_Atari_breakout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE\n",
    "\n",
    "### Get Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "rewards = []\n",
    "nb_frames = 500\n",
    "env = gym.make('Breakout-v0')\n",
    "env.reset()\n",
    "for t in range(nb_frames):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # take a random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frames.append(preprocess(observation))\n",
    "    if t%4 == 3 or done:\n",
    "        frameTensor = np.stack(frames)\n",
    "        minibatch = Variable(torch.Tensor(frameTensor))  ## convert to torch Variable data type\n",
    "        print('t:', t, '\\n', minibatch)\n",
    "        frames = []\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Preprocessed Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for frame in frames:\n",
    "    plt.imshow(frame, cmap = plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# EXPERIMENTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "# Atari emulator\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "# game variables\n",
    "nb_games = 5  ## number of games to play\n",
    "time_steps = 500  ## max number of time steps per game\n",
    "\n",
    "# experience replay variables\n",
    "N = int(1e6)  ## capacity\n",
    "D = deque()  ## deque object\n",
    "\n",
    "# RL vars\n",
    "anneal_tracker = 0  ## tally of how many total iterations have passed\n",
    "anneal_stop = 1000  ## nb of steps until annealing stops\n",
    "gen_epsilon = epsilon_generator(start=1, stop=0.1, num=anneal_stop)  ## Prob(choosing random action) w/linear annealing\n",
    "discount = 0.9  ## on future rewards\n",
    "\n",
    "# CNN setup\n",
    "cnn = CNN()\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(cnn.params, \n",
    "                          lr=learning_rate, \n",
    "                          alpha=0.99, \n",
    "                          eps=1e-08, \n",
    "                          weight_decay=0, \n",
    "                          momentum=0, \n",
    "                          centered=False)\n",
    "\n",
    "# algorithm\n",
    "for episode in range(nb_games):\n",
    "    gamestatus = 'nonterminal'\n",
    "    raw_frame = env.reset()  ## raw initial frame\n",
    "    seq_init = preprocess(raw_frame)  ## preprocessed initial sequence \n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            action = env.action_space.sample() # take a random action\n",
    "            #action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action w/max Q-value\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # preprocess new observation after action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('step:', t, '| gamestatus:', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            if D[i][4] == 'terminal':\n",
    "                target = D[i][2] + 100\n",
    "            else:\n",
    "                #target = sample[i][2] + discount*(to be completed)\n",
    "                target = D[i][2]\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2])\n",
    "        # SGD update\n",
    "        #update weights\n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        #print('final target =', target)\n",
    "    #print( (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    #print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "for episode in range(nb_games):\n",
    "    ## setup to check loss per episode\n",
    "    running_loss = 0.0\n",
    "    ## empty list to capture mini-batch of frames\n",
    "    frames = []  \n",
    "    ## default status to differentiate rewards (aka targets)\n",
    "    gamestatus = 'nonterminal'  \n",
    "    ## raw frame of game start\n",
    "    raw_frame = env.reset()  \n",
    "    ## preprocessed initial frame \n",
    "    seq_init = preprocess(raw_frame)  \n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            # feedforward for agent-based action\n",
    "            action_decision = Variable(torch.Tensor(seq_init))  ## setup for CNN\n",
    "            action_decision = cnn(action_decision.unsqueeze(0))  ## return optimal action\n",
    "            # take epsilon-greedy action (prob(epsilon) = random; else argmax(action))\n",
    "            action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action_decision.data.max()\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # gather feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # preprocess new observation post action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # mini-batch setup\n",
    "        if t%4 == 3  or done:\n",
    "            ## makes arrays callable to feed into CNN\n",
    "            frameTensor = np.stack(frames)  \n",
    "            ## convert Numpy Array --> PyTorch Tensor --> PyTorch Variable\n",
    "            frameTensor = Variable(torch.Tensor(frameTensor))  \n",
    "            print('t:', t, '\\n', frameTensor.shape)  ## should be 4x82x80 unless 'done'\n",
    "            ## clear mini-batch\n",
    "            frames = []  \n",
    "        else:\n",
    "            frames.append(seq_update)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('step:', t, '| gamestatus:', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            if D[i][4] == 'terminal':\n",
    "                #target = D[i][2] + (discount * )\n",
    "                target = D[i][2]\n",
    "            else:\n",
    "                target = D[i][2]\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2])\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feedforward\n",
    "        outputs = cnn(frameTensor)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        print('loss:', loss)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if t % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (episode + 1, t + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        \n",
    "        #print('final target =', target)\n",
    "    #print( (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    #print(D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
