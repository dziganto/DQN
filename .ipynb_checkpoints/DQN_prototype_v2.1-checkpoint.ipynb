{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from collections import deque\n",
    "import skimage.measure\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(rgb_tensor):\n",
    "    '''\n",
    "    Transforms 3D RGB numpy tensor: crop, convert to 2D grayscale, downsample, and convert to PyTorch tensor.\n",
    "    '''\n",
    "    crop = rgb_tensor[30:194,:,:]\n",
    "    grayscale = np.dot(crop[...,:3], [0.2989, 0.5870, 0.1140])  ## using Matlab's formula\n",
    "    downsample = skimage.measure.block_reduce(grayscale, (2,2), np.max)\n",
    "    return Variable(torch.Tensor(downsample).unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Select Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    '''Returns an action value which is an int in range [0,3]'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.time_ = 0\n",
    "        \n",
    "    def update_time(self):\n",
    "        self.time_ += 1\n",
    "        \n",
    "    def action(self):\n",
    "        if self.time_ == 0:\n",
    "            self.action_ = 1  ## start game by firing ball\n",
    "        else:\n",
    "            # take agent-based action every 4 time steps; else push action forward w/out agent computing\n",
    "            if self.time_%4 == 0:\n",
    "                if np.random.binomial(n=1, p=eg.epsilon_, size=1):\n",
    "                    self.action_ = env.action_space.sample()  ## take random action\n",
    "                else:\n",
    "                    self.action_ = cnn(initial_seq).data.max(1)[1][0]  ## take optimal action according to NN\n",
    "        return self.action_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    \n",
    "    dq_ = deque(maxlen=500)\n",
    "\n",
    "    def __init__(self, C):\n",
    "        self.capacity_ = C\n",
    "        \n",
    "    def add_experience(self, experience_tuple):\n",
    "        '''add new experience to experience replay'''\n",
    "        self.dq_.append(experience_tuple)\n",
    "        \n",
    "    def sample(self, capacity=32):\n",
    "        '''sample from experience replay'''\n",
    "        nb_items = len(self.dq_)\n",
    "        if nb_items > capacity:\n",
    "            idx = np.random.choice( nb_items, size=capacity, replace=False)\n",
    "        else:\n",
    "            idx = np.random.choice( nb_items, size=nb_items, replace=False)\n",
    "        return [self.dq_[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpsilonGenerator():\n",
    "    \n",
    "    def __init__(self, start, stop, steps):\n",
    "        self.epsilon_ = start\n",
    "        self.stop_ = stop\n",
    "        self.steps_ = steps\n",
    "        self.step_size_ = (self.epsilon_ - stop) / (self.steps_)\n",
    "        self.count_ = 1\n",
    "        \n",
    "    def epsilon_update(self):\n",
    "        '''generate next epsilon value'''\n",
    "        if (self.epsilon_ >= self.stop_ and self.count_ < self.steps_):\n",
    "            self.count_ += 1\n",
    "            self.epsilon_ -= self.step_size_\n",
    "        else:\n",
    "            self.epsilon_ = self.stop_\n",
    "            self.count_ += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 8, 4)  ## Conv2d(nChannels, filters, kernel, stride)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, 4)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 32 * 4 * 4)  ## reshape \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_games = 2  ## number of games to play\n",
    "time_steps = 500  ## max number of time steps per game\n",
    "record = 0\n",
    "view = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-29 08:53:17,160] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "# Atari emulator\n",
    "env = gym.make('Breakout-v0')\n",
    "# whether to record training\n",
    "if record:\n",
    "    env = wrappers.Monitor(env, \n",
    "                           directory='/Users/davidziganto/Data_Science/PyTorch/OpenAI_vids/breakout-experiment-1', \n",
    "                           video_callable=None, ## takes video when episode number is perfect cube\n",
    "                           force=True)\n",
    "\n",
    "# instantiate key classes\n",
    "cnn = CNN()\n",
    "er = ExperienceReplay(C=100)\n",
    "eg = EpsilonGenerator(start=1, stop=0.1, steps=1000)\n",
    "agent = Action()\n",
    "\n",
    "# setup variables\n",
    "discount = 0.9  \n",
    "learning_rate = 0.01\n",
    "\n",
    "# CNN setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(cnn.parameters(), \n",
    "                          lr=learning_rate, \n",
    "                          alpha=0.99, \n",
    "                          eps=1e-08, \n",
    "                          weight_decay=0, \n",
    "                          momentum=0, \n",
    "                          centered=False)\n",
    "\n",
    "# play game\n",
    "for episode in range(num_games):\n",
    "    \n",
    "    ## start/reset environment\n",
    "    initial_seq = preprocess(env.reset())\n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        ## show game in real-time\n",
    "        if view:\n",
    "            env.render()\n",
    "        \n",
    "        # take action (0=do nothing; 1=fire ball; 2=move right; 3=move left)\n",
    "        action = agent.action()\n",
    "        agent.update_time()\n",
    "        \n",
    "        # update epsilon\n",
    "        eg.epsilon_update()\n",
    "        \n",
    "        # get feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # preprocess new observation post action    \n",
    "        final_seq = preprocess(observation)\n",
    "        \n",
    "        # stop if no more moves\n",
    "        if done:\n",
    "            er.add_experience((initial_seq, action, reward, final_seq, 'terminal'))\n",
    "            break\n",
    "        else:\n",
    "            er.add_experience((initial_seq, action, reward, final_seq, 'nonterminal'))\n",
    "            \n",
    "        # get mini-batch sample from experience replay\n",
    "        minibatch = er.sample()\n",
    "        \n",
    "        \n",
    "                    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do...\n",
    "\n",
    "1) fix dataset and target code\n",
    "2) check neural net updating\n",
    "3) add loss function to show learning over time\n",
    "4) add GPU functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "        # create dataset - TWEAK STARTING HERE!!!\n",
    "        replay_size = len(minibatch)\n",
    "        data_list = [minibatch[i][0] for i in range(replay_size)]\n",
    "        data = Variable(torch.Tensor(data_list).unsqueeze(1))\n",
    "        \n",
    "        # create target variable\n",
    "        target_list = []\n",
    "        for i in range(replay_size):\n",
    "            if minibatch[i][4] == 'terminal':\n",
    "                target_list.append(minibatch[i][2])\n",
    "            else:\n",
    "                target_list.append(minibatch[i][2] + discount * \n",
    "                                   cnn(minibatch[i][3].unsqueeze(0).unsqueeze(0)).data.max(1)[1][0])\n",
    "        targets = Variable(torch.Tensor(target_list))\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feedforward pass\n",
    "        outputs = cnn(data).max(1)[0]\n",
    "        ##print(outputs)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        #print('loss:', loss)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # set new observation as initial sequence\n",
    "        initial_seq = final_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats for 1000 games taking random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "rewards = []\n",
    "steps = []\n",
    "\n",
    "for game in range(1000):\n",
    "    \n",
    "    myreward = 0\n",
    "    \n",
    "    # reset game\n",
    "    env.reset()\n",
    "    \n",
    "    for t in range(1000):\n",
    "        \n",
    "        # show in real-time\n",
    "        #env.render()\n",
    "        \n",
    "        # take a random action\n",
    "        if t == 0:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = env.action_space.sample() \n",
    "        \n",
    "        # get feedback\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        myreward += reward\n",
    "        \n",
    "        # end game when out of balls\n",
    "        if done:\n",
    "            rewards.append(myreward)\n",
    "            steps.append(t)\n",
    "            stats = zip(steps, rewards)\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(rewards);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        #env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        ##print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            # feedforward for agent-based action\n",
    "            sample_frame = Variable(torch.Tensor(seq_init).unsqueeze(0).unsqueeze(0))  ## setup for CNN (unsqueeze to fake 4D tensor since single observation)\n",
    "            action_decision = cnn(sample_frame)  ## return optimal action \n",
    "            ##print(action_decision)\n",
    "            # take epsilon-greedy action (prob(epsilon) = random; else argmax(action))\n",
    "            #action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action_decision.data.max(1)[1][0]\n",
    "            if np.random.binomial(n=1, p=epsilon, size=1):\n",
    "                action = env.action_space.sample()\n",
    "            else: \n",
    "                checker += 1\n",
    "                action = action_decision.data.max(1)[1][0]\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # gather feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        # preprocess new observation post action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # mini-batch setup\n",
    "        if t%4 == 3 or done:\n",
    "            ##print(t)\n",
    "            frames.append(seq_update)\n",
    "            ## makes arrays callable to feed into CNN\n",
    "            frameTensor = np.stack(frames)  \n",
    "            ## convert Numpy Array --> PyTorch Tensor --> PyTorch Variable\n",
    "            frameTensor = Variable(torch.Tensor(frameTensor))  \n",
    "            ##print('t:', t, '\\n', frameTensor)  ## should be 4x82x80 unless 'done'\n",
    "            ## clear mini-batch\n",
    "            frames = []  \n",
    "        else:\n",
    "            frames.append(seq_update)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            ##print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "            ##      '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            print('steps:', t, '| episode:', episode, '| score:', score, '| checker:', checker)\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            ##print('step:', t, '| gamestatus:', gamestatus, '| action:', action, '| len(D):', len(D), \n",
    "            ##      '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        ## empty list to capture mini-batch of D\n",
    "        minibatch_D = []\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            minibatch_D.append(D[i])\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2]) \n",
    "            \n",
    "        # create dataset\n",
    "        data_list = [D[i][0] for i in range(D_size)]\n",
    "        data = Variable(torch.Tensor(data_list).unsqueeze(1))\n",
    "        ##print(data)\n",
    "        \n",
    "        # create target variable\n",
    "        target_list = []\n",
    "        for i in range(D_size):\n",
    "            if D[i][4] == 'terminal':\n",
    "                target_list.append(D[i][2])\n",
    "            else:\n",
    "                target_list.append(D[i][2] + discount * \n",
    "                                   cnn(Variable(torch.Tensor(D[i][3]).unsqueeze(0).unsqueeze(0))).data.max(1)[1][0])\n",
    "        targets = Variable(torch.Tensor(target_list))\n",
    "        ##print(targets)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feedforward pass\n",
    "        outputs = cnn(data).max(1)[0]\n",
    "        ##print(outputs)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        #print('loss:', loss)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        \n",
    "        # print statistics\n",
    "        #running_loss += loss.data[0]\n",
    "        #if t % 200 == 199:    # print every 200 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (episode + 1, t + 1, running_loss / 200))\n",
    "        #    running_loss = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check how many times DQN chose action as opposed to random action\n",
    "checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensure minibatch of experience replay doesn't exceed 32\n",
    "len(minibatch_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(cnn.state_dict(), '/Users/davidziganto/Data_Science/PyTorch/DL_models/DL_RL_Atari_breakout_500e_10000t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn = CNN()\n",
    "#cnn.load_state_dict(torch.load('/Users/davidziganto/Data_Science/PyTorch/DL_models/DL_RL_Atari_breakout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE\n",
    "\n",
    "### Get Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "rewards = []\n",
    "nb_frames = 500\n",
    "env = gym.make('Breakout-v0')\n",
    "env.reset()\n",
    "for t in range(nb_frames):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # take a random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frames.append(preprocess(observation))\n",
    "    if t%4 == 3 or done:\n",
    "        frameTensor = np.stack(frames)\n",
    "        minibatch = Variable(torch.Tensor(frameTensor))  ## convert to torch Variable data type\n",
    "        print('t:', t, '\\n', minibatch)\n",
    "        frames = []\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Preprocessed Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for frame in frames:\n",
    "    plt.imshow(frame, cmap = plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# EXPERIMENTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "# Atari emulator\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "# game variables\n",
    "nb_games = 5  ## number of games to play\n",
    "time_steps = 500  ## max number of time steps per game\n",
    "\n",
    "# experience replay variables\n",
    "N = int(1e6)  ## capacity\n",
    "D = deque()  ## deque object\n",
    "\n",
    "# RL vars\n",
    "anneal_tracker = 0  ## tally of how many total iterations have passed\n",
    "anneal_stop = 1000  ## nb of steps until annealing stops\n",
    "gen_epsilon = epsilon_generator(start=1, stop=0.1, num=anneal_stop)  ## Prob(choosing random action) w/linear annealing\n",
    "discount = 0.9  ## on future rewards\n",
    "\n",
    "# CNN setup\n",
    "cnn = CNN()\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(cnn.params, \n",
    "                          lr=learning_rate, \n",
    "                          alpha=0.99, \n",
    "                          eps=1e-08, \n",
    "                          weight_decay=0, \n",
    "                          momentum=0, \n",
    "                          centered=False)\n",
    "\n",
    "# algorithm\n",
    "for episode in range(nb_games):\n",
    "    gamestatus = 'nonterminal'\n",
    "    raw_frame = env.reset()  ## raw initial frame\n",
    "    seq_init = preprocess(raw_frame)  ## preprocessed initial sequence \n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            action = env.action_space.sample() # take a random action\n",
    "            #action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action w/max Q-value\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # preprocess new observation after action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('step:', t, '| gamestatus:', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            if D[i][4] == 'terminal':\n",
    "                target = D[i][2] + 100\n",
    "            else:\n",
    "                #target = sample[i][2] + discount*(to be completed)\n",
    "                target = D[i][2]\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2])\n",
    "        # SGD update\n",
    "        #update weights\n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        #print('final target =', target)\n",
    "    #print( (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    #print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "for episode in range(nb_games):\n",
    "    ## setup to check loss per episode\n",
    "    running_loss = 0.0\n",
    "    ## empty list to capture mini-batch of frames\n",
    "    frames = []  \n",
    "    ## default status to differentiate rewards (aka targets)\n",
    "    gamestatus = 'nonterminal'  \n",
    "    ## raw frame of game start\n",
    "    raw_frame = env.reset()  \n",
    "    ## preprocessed initial frame \n",
    "    seq_init = preprocess(raw_frame)  \n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        # show game in real-time\n",
    "        env.render()\n",
    "        \n",
    "        # linearly anneal epsilon (prob of selecting random action)\n",
    "        if anneal_tracker <= anneal_stop:\n",
    "            epsilon = next(gen_epsilon)\n",
    "        print('epsilon:', epsilon)\n",
    "        anneal_tracker += 1\n",
    "        \n",
    "        # take agent-based action every 4 time steps; otherwise push action forward w/out agent computing\n",
    "        if t%4 == 0:\n",
    "            # feedforward for agent-based action\n",
    "            action_decision = Variable(torch.Tensor(seq_init))  ## setup for CNN\n",
    "            action_decision = cnn(action_decision.unsqueeze(0))  ## return optimal action\n",
    "            # take epsilon-greedy action (prob(epsilon) = random; else argmax(action))\n",
    "            action = env.action_space.sample() if np.random.binomial(n=1, p=epsilon, size=1) else action_decision.data.max()\n",
    "            #print('action =', action)\n",
    "        \n",
    "        # gather feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # preprocess new observation post action    \n",
    "        seq_update = preprocess(observation)\n",
    "        \n",
    "        # mini-batch setup\n",
    "        if t%4 == 3  or done:\n",
    "            ## makes arrays callable to feed into CNN\n",
    "            frameTensor = np.stack(frames)  \n",
    "            ## convert Numpy Array --> PyTorch Tensor --> PyTorch Variable\n",
    "            frameTensor = Variable(torch.Tensor(frameTensor))  \n",
    "            print('t:', t, '\\n', frameTensor.shape)  ## should be 4x82x80 unless 'done'\n",
    "            ## clear mini-batch\n",
    "            frames = []  \n",
    "        else:\n",
    "            frames.append(seq_update)\n",
    "        \n",
    "        # stop if out of lives\n",
    "        if done:\n",
    "            gamestatus = 'terminal'\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('*step: ', t, '| gamestatus: ', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:', (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "            break\n",
    "        else:\n",
    "            # update experience replay\n",
    "            experience_replay(C=N, DQ = D, seq_init=seq_init, \n",
    "                              action=action, reward=reward, \n",
    "                              seq_update=seq_update, gamestatus=gamestatus)\n",
    "            print('step:', t, '| gamestatus:', gamestatus, '| len(D):', len(D), \n",
    "                  '| init != update:',(D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    \n",
    "        \n",
    "        # mini-batch sample of experience replay for ConvNet\n",
    "        D_size = len(D)\n",
    "        idx = np.random.choice(range(D_size), size=min(D_size, 32), replace=False)\n",
    "        # calculate target\n",
    "        for i in idx:\n",
    "            if D[i][4] == 'terminal':\n",
    "                #target = D[i][2] + (discount * )\n",
    "                target = D[i][2]\n",
    "            else:\n",
    "                target = D[i][2]\n",
    "            #print('step: ', i, 'gamestatus: ', D[4], 'reward: ', D[2])\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feedforward\n",
    "        outputs = cnn(frameTensor)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        print('loss:', loss)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # set new observation as initial sequence\n",
    "        seq_init = seq_update\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if t % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (episode + 1, t + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        \n",
    "        #print('final target =', target)\n",
    "    #print( (D[len(D)-1][0] != D[len(D)-1][3]).sum())\n",
    "    #print(D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
